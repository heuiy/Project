#!/usr/bin/env python
# coding: utf-8

# # [Case2-5] ë°˜ë„ì²´ ê³µì • ë°ì´í„°ë¥¼ í™œìš©í•œ ê³µì • ì´ìƒ ì˜ˆì¸¡_Rev.01

# ---

# 
# ## í”„ë¡œì íŠ¸ ëª©í‘œ
# ---
# - ë‹¤ì¸µí¼ì…‰íŠ¸ë¡  ê¸°ë°˜ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ êµ¬í˜„.
# - ê²°ì¸¡ê°’ ì²˜ë¦¬, íŠ¹ì§ˆ ì„ íƒ, íŠ¹ì§ˆ ì¶”ì¶œ ë“± ì •í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—….
# - í•™ìŠµ ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œ ì™„í™” ê¸°ë²•.
# - ë‹¤ì¸µí¼ì…‰íŠ¸ë¡  ë¶„ë¥˜ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¸°ë²•.

# ## í”„ë¡œì íŠ¸ ëª©ì°¨
# ---
# 
# 1. **ì •í˜• ë°ì´í„° ì½ê¸°:** Localì— ì €ì¥ë˜ì–´ ìˆëŠ” ì •í˜• ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í™•ì¸
# 
# 2. **ë°ì´í„° ì „ì²˜ë¦¬:** ëª¨ë¸ì— í•„ìš”í•œ ì…ë ¥ í˜•íƒœë¡œ ë°ì´í„° ì²˜ë¦¬
# 
# 3. **ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ëª¨ë¸ ì •ì˜ :** ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ëª¨ë¸ êµ¬í˜„
# 
# 4. **í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì • ë° ì»´íŒŒì¼ :** ì˜¬ë°”ë¥¸ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •
# 
# 5. **ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰:** <span style="color:red"> ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  </span>ì„ í†µí•œ í•™ìŠµ ìˆ˜í–‰, í‰ê°€ ë° ì˜ˆì¸¡ ìˆ˜í–‰
# 
# 6. **ì œì¶œ:** ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ ì œì¶œí•œ í›„ ì±„ì  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

# ## ë°ì´í„° ì¶œì²˜
# ---
# https://www.kaggle.com/paresh2047/uci-semcom

# ## í”„ë¡œì íŠ¸ ê°œìš”
# ---
# 
# **ë°ì´í„°:** ì„¼ì„œ ë° ì¸¡ì •ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë°˜ë„ì²´ ì œì¡° ê³µì • ì •ë³´ì™€ ê·¸ì— ë”°ë¥¸ ê³µì • ì´ìƒ ì—¬ë¶€ ë°ì´í„°.
# 
# **ê°€ì •:** ì¸¡ì •ëœ ì œì¡° ê³µì • ì •ë³´ì— ê²°í•¨ ì—¬ë¶€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ” íŠ¹ì§ˆì´ ì¡´ì¬.
# 
# **ëª©í‘œ:** ê²°í•¨ì´ ìˆëŠ” ë°˜ë„ì²´ë¥¼ ì¶œê³ ì „ì— ì œì™¸.
# 
# **ì„¤ëª…:** 
# 
# ```
# ë°ì´í„°ì—ëŠ” ê²°ì¸¡ê°’ì´ ì¡´ì¬.
# ì¸¡ì •ëœ ì‹œê·¸ë„(íŠ¹ì§ˆ)ë“¤ì€ ë§¤ìš° ë‹¤ì–‘í•˜ì§€ë§Œ (590ê°œ) ë¶ˆí•„ìš”í•œ ì •ë³´ë‚˜ ë…¸ì´ì¦ˆë¥¼ í¬í•¨.
# ì†ŒëŸ‰ì˜ í•™ìŠµ ë°ì´í„° ì œê³µ(ì´ 1567ê°œ).
# í´ë˜ìŠ¤ê°„ í•™ìŠµ ë°ì´í„° ìˆ˜ ë¶ˆê· í˜•í•¨ (ì´ìƒì—†ìŒ: 1463ê°œ, ì´ìƒ:104ê°œ)
# ```

# ## ê²°ê³¼ ìš”ì•½
# 
# ê° íŒŒë¼ë¯¸í„° ì¡°ê±´ì— ë”°ë¥¸ ëª¨ë¸ í‰ê°€ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.
# 
# |Run|Epochs|Scaler|criterion|frac|max_iter|preds>|Dense|criterion|n_trees|max_depth|n_batches|epoch|ë³€ìˆ˜|lr|class0|class1|
# |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
# |01|-|Standard||||0.5|30|||||||0.001|0.938356|0.142857|
# |02|-|Robust||||0.5|30|||||||0.001|0.939130|0.313725|
# |03|-|Robust||||0.5|30|||||||0.0001|0.908766|0.238806|
# |04|-|Robust||||0.5|30|||||||0.01|0.943201|0.266667|
# |05|-|Robust||||0.8|30|||||||0.001|0.947368|0.162162|
# |06|-|Robust||||0.5|100|||||||0.001|0.944828|0.304348|
# 
# class 0 / class 1ì˜ f1-scoreê°€ ê°ê° 0.93 / 0.34 ì´ìƒì´ë©´ 100ì 
# 
# ëª©í‘œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ ì–´ë–¤ ë°©ë²•ì„ í™œìš©í•˜ì—¬ë„ ì¢‹ìŠµë‹ˆë‹¤.
# 
# - ë°ì´í„° ìŠ¤ì¼€ì¼ë§
# - upsampling ë°©ë²• ì¡°ì • ë“±

# ## 1. ë°ì´í„° ì½ê¸° 
# ---

# ### 1.1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°

# In[ ]:


# ë°ì´í„° í”„ë ˆì„ í˜•íƒœì˜ ë°ì´í„° ë° ì—°ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import random
import numpy as np
import pandas as pd

# ì‹œê°í™” ë° í•™ìŠµ ë¡œê·¸ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib.pyplot as plt
import seaborn as sns


# In[ ]:


# ëª¨ë¸ ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ì‹œë“œ ì„¤ì •
random.seed(42)
np.random.seed(42)


# ### 1.2 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ---
# ë°ì´í„° í”„ë ˆì„ í˜•íƒœì˜ ë°ì´í„°ë¥¼ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•´ ë¶ˆëŸ¬ì˜¤ì.

# In[ ]:


df = pd.read_csv("secom.data", sep=" ", names = [str(i)+"th col" for i in range(590)])


# In[ ]:


df.head()


# In[ ]:


df.shape # ë°ì´í„° í˜•íƒœ í™•ì¸.


# ## 2. ë°ì´í„° ì •ì œ
# ---

# ### 2.1 ë°ì´í„° ê²°ì¸¡ì¹˜ ë³´ê°„

# In[ ]:


df = df.drop([], axis=1)


# In[ ]:


df = df.fillna(df.mean())


# In[ ]:


df


# ## 3. ë°ì´í„° ì‹œê°í™”

# ### 3.1 ë°ì´í„° ì •ì œ
# 
# - ë°ì´í„°ì˜ í†µê³„ì¹˜ (ê°œìˆ˜, í‰ê· , ë¶„ì‚°, í‘œì¤€í¸ì°¨) í™•ì¸. 

# In[ ]:


df.describe()


# - ë°ì´í„°ì˜ ìƒê´€ ê´€ê³„ ë¶„ì„

# ## 4. ë°ì´í„° ì „ì²˜ë¦¬

# In[ ]:


labels = pd.read_csv("secom_labels.data", sep=" ", names = ['label', 'date'])


# In[ ]:


labels = labels.drop(['date'], axis=1)
labels = labels.replace(-1, 0)


# In[ ]:


labels


# ### 4.1 í•™ìŠµ/í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬

# In[ ]:


from sklearn.model_selection import train_test_split


# In[ ]:


train_dataset = df.sample(frac=0.8,random_state=0)
test_dataset = df.drop(train_dataset.index)
train_labels = labels.loc[train_dataset.index]
test_labels = labels.drop(train_dataset.index)

train_stats = train_dataset.describe()
train_stats = train_stats.transpose()


# In[ ]:


print("train data shape : ", train_dataset.shape)
print("test data shape : ", test_dataset.shape)


# In[ ]:


train_stats


# ### 4.2 íŠ¹ì„± ìŠ¤ì¼€ì¼ë§

# In[ ]:


from sklearn.preprocessing import StandardScaler # í‘œì¤€ ìŠ¤ì¼€ì¼ë§  (í‰ê·  = 0 / í‘œì¤€í¸ì°¨ = 1)
from sklearn.preprocessing import MinMaxScaler  # ìµœëŒ€/ìµœì†Œ ìŠ¤ì¼€ì¼ë§ (ì´ìƒì¹˜ì— ì·¨ì•½)
from sklearn.preprocessing import RobustScaler  # ì¤‘ì•™ê°’ = 0 / IQR(1ë¶„ìœ„(25%) ~ 3ë¶„ìœ„(75%)) = 1 (ì´ìƒì¹˜ ì˜í–¥ ìµœì†Œí™”, ë„“ê²Œ ë¶„í¬)
from sklearn.preprocessing import MaxAbsScaler  # |x|  <= 1 , ì´ìƒì¹˜ì— ì·¨ì•½í•  ìˆ˜ ìˆë‹¤. ì–‘ìˆ˜ë§Œ ìˆëŠ” ë°ì´í„°ì˜ ê²½ìš° MinMaxScaler ìœ ì‚¬


# In[ ]:


# st_scaler = StandardScaler()
st_scaler = RobustScaler()

normed_train_data = st_scaler.fit_transform(train_dataset)
normed_test_data = st_scaler.fit_transform(test_dataset)
normed_train_data = pd.DataFrame(normed_train_data)
normed_test_data = pd.DataFrame(normed_test_data)


# In[ ]:


normed_train_data.head() # ìŠ¤ì¼€ì¼ë§ ëœ ê°’ í™•ì¸.


# In[ ]:


# ê²°ì¸¡ì¹˜ ê°’ ì œê±°.
normed_train_data = normed_train_data.dropna(axis=1)
normed_test_data = normed_test_data.dropna(axis=1)


# In[ ]:


# ë°ì´í„° íŠ¹ì„±ê°’ ë²”ìœ„ í™•ì¸

import matplotlib.pyplot as plt
import matplotlib


# matplotlib ì„¤ì •
matplotlib.rc('font', family='AppleGothic') # í•œê¸€ ì„¤ì •
plt.rcParams['axes.unicode_minus'] = False # -í‘œì‹œ

# feature visualization
plt.boxplot(normed_test_data[50], manage_ticks=False) # ë°ì´í„°, ì†Œëˆˆê¸ˆ í‘œì‹œ ì•ˆí•˜ê¸°
plt.yscale('symlog') # ì¶• ìŠ¤ì¼€ì¼ì„ log ë¡œ
plt.xlabel('feature list') # xì¶• ì´ë¦„
plt.ylabel('feature') # yì¶• ì´ë¦„
plt.show() # ê·¸ë˜í”„ ì¶œë ¥


# ### 4.3 í´ë˜ìŠ¤ ë¶ˆê· í˜• ì™„í™” ê¸°ë²•
# 
# - ì˜¤ë²„ ìƒ˜í”Œë§ : í•™ìŠµ ê³¼ì • ë™ì•ˆ ë°ì´í„°ê°€ ì ì€ í´ë˜ìŠ¤ì—ì„œ ì˜ë„ì ìœ¼ë¡œ ë” ìì£¼ í‘œë³¸ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ êµ¬ì„±.
# - ì–¸ë” ìƒ˜í”Œë§ : í•™ìŠµ ê³¼ì • ë™ì•ˆ ë°ì´í„°ê°€ ë§ì€ í´ë˜ìŠ¤ì—ì„œ í‘œë³¸ì„ ì˜ë„ì ìœ¼ë¡œ ë” ì ê²Œ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ êµ¬ì„±.

# In[ ]:


get_ipython().system('pip install imblearn')


# In[ ]:


from sklearn.datasets import make_classification
from imblearn.over_sampling import RandomOverSampler


# In[ ]:


ros = RandomOverSampler(random_state=0)
X_resampled, y_resampled = ros.fit_resample(normed_train_data, train_labels)


# In[ ]:


cnt_zero = 0
cnt_one = 0
for l in y_resampled.values:
    if l == 1:
        cnt_one+=1
    else:
        cnt_zero+=1

print(f"label ratio -> {cnt_zero} : {cnt_one}")


# ## 5. ë”¥ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸

# ### 5.5 ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ëª¨ë¸ êµ¬í˜„
# 
# - ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ëŠ” ë‹¤ë¥´ê²Œ ì¸í’‹ê³¼ ì•„ì›ƒí’‹ ì‚¬ì´ì˜ ìˆ¨ê²¨ì§„(hidden) ë¹„ì„ í˜• ë ˆì´ì–´ë“¤ì„ í•˜ë‚˜ ì´ìƒ í¬í•¨í•˜ì—¬ mì°¨ì›ì˜ ë°ì´í„°ë¥¼ o ì›í•˜ëŠ” ìˆ˜ì˜ ì•„ì›ƒí’‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë‚´ë„ë¡ í•™ìŠµ. 
# - ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° 0ê³¼ 1 ì‚¬ì´ë¡œ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡ í•™ìŠµ.
# - ë§¤ìš° ë³µì¡í•œ ë¹„ì„ í˜• ëª¨ë¸ë„ í‘œí˜„ê°€ëŠ¥í•˜ë¯€ë¡œ ë³µì¡í•œ ë°ì´í„° ë¶„í¬ë„ í•™ìŠµ ê°€ëŠ¥í•¨.

# In[ ]:


import tensorflow as tf
from tensorflow import keras    
from tensorflow.keras import layers


# In[ ]:


# í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì •ì˜
lr = 0.001
training_epoch = 10

# í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜.
auc = tf.keras.metrics.AUC(num_thresholds=3)
precision = tf.keras.metrics.Precision()
recall = tf.keras.metrics.Recall()
tp = tf.keras.metrics.TruePositives()
fn = tf.keras.metrics.FalseNegatives()


# In[ ]:


def logistic_regression():
    model = keras.Sequential([
    layers.Dense(1, activation='sigmoid', input_shape=[normed_train_data.shape[1]]) # ê²°ì¸¡ ì—´ ì œê±°.
  ])
    
    
    optimizer = tf.keras.optimizers.RMSprop(lr)
    loss_object = tf.keras.losses.BinaryCrossentropy()
    
    model.compile(loss=loss_object,
                optimizer=optimizer,
                metrics=['accuracy', auc, precision, recall, tp, fn])
    return model

logi_model = logistic_regression()


# In[ ]:


def multi_layer_perceptron():
    model = keras.Sequential([
    layers.Dense(100, activation='relu', input_shape=[normed_train_data.shape[1]]), # len(train_dataset.keys())ê²°ì¸¡ ì—´ ì œê±°.
    # layers.Dense(100, activation='relu'),
    layers.Dense(1, activation='sigmoid')
  ])
    
    optimizer = tf.keras.optimizers.RMSprop(lr)
    loss_object = tf.keras.losses.BinaryCrossentropy()
    
    model.compile(loss=loss_object,
                optimizer=optimizer,
                metrics=['accuracy', auc, precision, recall, tp, fn])
    return model

mlp_model = multi_layer_perceptron()


# In[ ]:


mlp_model.summary()


# #### 5.5.1 í•™ìŠµ

# In[ ]:


early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)


# In[ ]:


mlp_model.fit(normed_train_data, train_labels,
              validation_split = 0.2, epochs=training_epoch, verbose=2, callbacks=[early_stop])


# #### 5.5.2 ì˜ˆì¸¡ ë° í‰ê°€
# 
# ë¶ˆê· í˜•í•œ ë°ì´í„°ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´, Accuracy ì´ì™¸ì— ì •ë°€ë„, ì¬í˜„ìœ¨, ì´ ë‘˜ì˜ ì¡°í™”í‰ê· ì¸ F1 scoreë¥¼ êµ¬í•¨.
# 
# - precision : ëª¨ë¸ì´ positiveë¼ê³  ë¶„ë¥˜í•œ ê²ƒë“¤ ì¤‘, ì§„ì§œ positive ë°ì´í„° ë¹„ìœ¨. tp / tp + fp
# - recall : ëª¨ë¸ì´ ë§ê²Œ ë¶„ë¥˜í•œ ê²ƒë“¤ ì¤‘, positiveë¥¼ ë°ì´í„°ë¥¼ ë§ì¶˜ ë¹„ìœ¨. tp / tp + fn
# - f1 score : 2 * precision * recall / precision + recall 
# 
# í´ë˜ìŠ¤ê°€ ë¶ˆê· í˜•í•  ìˆ˜ë¡ ì •í™•ë„ ì´ì™¸ì˜ ìœ„ì™€ê°™ì€ ì§€í‘œë“¤ì„ ê°™ì´ ì‚´í´ë³´ì•„ì•¼ ë¶„ë¥˜ê¸° ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ì¸¡ì • ê°€ëŠ¥.
# 
# - roc curve : xì¶•ì„ False Positive Rate yì¶•ì„ Recall(True Positive Rate)ë¡œ ë‘ê³  ì‹œê°í™”í•œ ê·¸ë˜í”„.
# - roc curveì˜ ì•„ë˜ë©´ì ì„ AUCë¡œ ì»¤ë¸Œì˜ ê¼­ì§€ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì´ ì¢‹ìŒì„ ì˜ë¯¸.
# 
# - BER : ê° í´ë˜ìŠ¤ì˜ ì˜¤ë¥˜ìœ¨ì˜ í‰ê· ìœ¼ë¡œ 0ì¼ë•Œ ì™„ë²½í•œ ë¶„ë¥˜ê¸°, 0.5ì¼ë•Œ ëœë¤ ì„ íƒ ìˆ˜ì¤€ì˜ ë¶„ë¥˜ê¸°.
#     - 1/2(ğ¹ğ‘ğ‘…+ğ¹ğ‘ƒğ‘…)

# In[ ]:


scores = mlp_model.evaluate(normed_test_data, test_labels, verbose=2)


# In[ ]:


from sklearn.metrics import confusion_matrix, classification_report

predict = mlp_model.predict(normed_test_data)
predict = np.where(predict > 0.5, 1, 0)
print(classification_report(predict, test_labels))


# In[ ]:


import sklearn.metrics as metrics
import matplotlib.pyplot as plt


# In[ ]:


# preds = mlp_model.predict_proba(normed_test_data)
preds = mlp_model.predict(normed_test_data)


# In[ ]:


fpr, tpr, threshold = metrics.roc_curve(test_labels, preds)
roc_auc = metrics.auc(fpr, tpr)

plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()


# #### 5.5.3 ìƒ˜í”Œë§ ë°ì´í„° í•™ìŠµ
# 

# In[ ]:


mlp_model = multi_layer_perceptron()


# In[ ]:


mlp_model.fit(X_resampled, y_resampled,
              validation_split = 0.2, epochs=training_epoch, verbose=2, callbacks=[early_stop])


# In[ ]:


scores = mlp_model.evaluate(normed_test_data, test_labels, verbose=2)


# In[ ]:


preds = mlp_model.predict(normed_test_data)


# In[ ]:


from sklearn.metrics import confusion_matrix, classification_report

preds = np.where(preds > 0.5, 1, 0)
print(classification_report(preds, test_labels))

res = classification_report(preds, test_labels, output_dict=True)


# In[ ]:


fpr, tpr, threshold = metrics.roc_curve(test_labels, preds)
roc_auc = metrics.auc(fpr, tpr)

plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()


# #### 5.5.3 ëª¨ë¸ ì•™ìƒë¸”

# In[ ]:


num_ensemble_models = 2
training_epoch = 3

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

# ëª¨ë¸ì„ ì—¬ëŸ¬ê°œë¥¼ ìƒì„±í•´ì„œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¦.
models = [] 
for m in range(num_ensemble_models):
    models.append(multi_layer_perceptron())


# In[ ]:


# ëª¨ë¸ë¦¬ìŠ¤íŠ¸ì—ì„œ ì •ì˜ëœ ëª¨ë¸ì„ í•˜ë‚˜ì”© ê°€ì ¸ì™€ì„œ í•™ìŠµì„ ì§„í–‰í•¨.
for i, model in enumerate(models):
    model.fit(normed_train_data, train_labels,
              validation_split = 0.2, epochs=training_epoch, verbose=2, callbacks=[early_stop])
    print("model %d completion!" % i)
    # í•™ìŠµì´ ëë‚œ ëª¨ë¸ì„ ì €ì¥í•¨.
    model.save_weights("my_checkpoint"+str(i))
    


# In[ ]:


pred_list = []
for i, model in enumerate(models):
    pred = model.predict(normed_test_data)
    pred_list.append(pred)
    
predictions = sum(pred_list)/len(pred_list)
predictions = np.where(predictions > 0.5, 1, 0)

print(classification_report(predictions, test_labels))


# #### 5.5.3 customized thresholding

# In[ ]:


pred_list


# In[ ]:


pred_list = []
for i, model in enumerate(models):
    pred = model.predict(normed_test_data)
    pred_list.append(pred)
    
predictions = sum(pred_list)/len(pred_list)
predictions = np.where(predictions > 0.5, 1, 0)

print(classification_report(predictions, test_labels))


# ## 6. ì œì¶œ

# â€» ê³¼ì œ ì œì¶œ ê´€ë ¨í•˜ì—¬ jupyter notebook ìµœìƒë‹¨ì˜ `randomseed(=42)`ë¥¼ ì ˆëŒ€ ìˆ˜ì •í•˜ì‹œ ë§ˆì„¸ìš”
# 
# ---
# 
# ë‹¤ì¸µí¼ì…‰íŠ¸ë¡  ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬í˜„í•˜ì—¬ í‘œì¤€ ìŠ¤ì¼€ì¼ë§ ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°(**normed_test_data**)ë¥¼ ì¶”ë¡ í•´ë³´ì„¸ìš”.
# 
# ëª©í‘œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ `ë°ì´í„° ìŠ¤ì¼€ì¼ë§`, `upsampling ë°©ë²•` ì¡°ì • ë“± ì–´ë–¤ ë°©ë²•ì„ í™œìš©í•˜ì—¬ë„ ì¢‹ìŠµë‹ˆë‹¤.
# 
# ì¶”ë¡  ê²°ê³¼ë¥¼ ì•„ë˜ í‘œì™€ ê°™ì€ í¬ë§·ì˜ csv íŒŒì¼ë¡œ ì €ì¥í•´ì£¼ì„¸ìš”.
# 
# |  | label |
# |-------|------|
# | 0     | 0 |
# | 1     | 1 |
# | 2     | 1 |
# | 3     | 0 |
# | 4     | 0 |
# 
# ìœ„ì²˜ëŸ¼, í…ŒìŠ¤íŠ¸ ë°ì´í„°(**normed_test_data**)ì™€ ê°™ì€ ìˆœì„œë¡œ ì •ë ¬ëœ `index`ì™€ ê·¸ì— ëŒ€í•œ `label`ì„ ì—´ë¡œ ê°–ëŠ” dataframeì„ `submission.csv` ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
# 
# `sklearn.metrics`ì˜ `classification_report`ì„ í†µí•œ ì„±ëŠ¥ ì¸¡ì •ìœ¼ë¡œ **class 0 / class 1ì˜ f1-score**ê°€ ê°ê° **0.93 / 0.34** ì´ìƒì´ë©´ 100ì ì…ë‹ˆë‹¤.
# 
# (ë¶€ë¶„ì ìˆ˜ ìˆìŒ)
# 
# 

# ### ì±„ì 

# ê²°ê³¼ csv íŒŒì¼ì„ ì €ì¥ í›„, ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì±„ì ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# 
# **ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ë©´ ì±„ì ì´ ë¶ˆê°€ëŠ¥ í•©ë‹ˆë‹¤.**

# In[ ]:


# ì œì¶œí•  dataframeì„ ì•„ë˜ ì½”ë“œì— ëŒ€ì…í•˜ì—¬ submission.csv íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

answer_df = pd.DataFrame(preds)
answer_df.columns = ['label']

print(answer_df)
answer_df.to_csv('submission.csv', index=False)


# In[ ]:


# ì±„ì ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•˜ì—¬ ë¡œê·¸ì¸
import sys
sys.path.append('vendor')
from elice_challenge import check_score, upload


# In[ ]:


# ì œì¶œ íŒŒì¼ ì—…ë¡œë“œ
await upload()


# In[ ]:


# ì±„ì  ìˆ˜í–‰
await check_score()

